### Academic statement (MIT)

My main interest in neuroscience is the relationship between neural coding and memory: how neural circuits use efficient representations to reliably store and retrieve information, in spite of the formidable constraints imposed on them by biology. Understanding how the brain encodes and maintains internal state is essential if we want to model phenomena that cannot be explained as simple mappings from sensory inputs to behavioral outputs. My goal in research, at least for the near future, is to apply my background in computer science and statistics to the study of neural coding and memory, to develop both tools for analyzing experimental data and models of neural computations.

My first exposure to research at the intersection of neuroscience and machine learning was working on my undergraduate thesis with Prof. Jonathan Pillow. My project focused on making a black-box optimization algorithm called Bayesian optimization more useful for fitting statistical models to neural data. The models we were concerned with could only be fit by maximizing extremely complicated likelihood functions that were effectively black boxes. Bayesian optimization recasts this optimization problem as one of inference: the function being optimized is assumed to be drawn from a Gaussian process prior, which, when combined with observations of the function's values at a set of chosen test points, yields a posterior distribution over possible function values and an expected optimum value. Many strategies exist for choosing test points, but none of them work well on the likelihood functions we were trying to optimize, where the level of noise in each function observation is not constant, but dependent on the input value. For my thesis project, I used a result from the experimental design literature to create a new strategy for test point selection specifically tailored to these likelihood functions, and demonstrated its efficacy using numerical simulations. The experience of working in this lab provided me with an invaluable introduction to the models and tools for analyzing neural data, as well as the field at large.

In May of this year, I joined Prof. Cristina Savin's lab at NYU as a research assistant, to work on computational modeling of memory, plasticity, and learning. So far, I have worked primarily on using Gaussian process models to infer tuning curves from neural recordings. While Gaussian process models have existed for a long time, algorithms that tractably fit them to large datasets have only recently been introduced, allowing us for the first time to use them to analyze many types of neural data. For the first project I worked on, starting in June, we tried to apply this approach to calcium imaging data recorded from the CA3 hippocampal region in rodents performing a spatial navigation task. Unfortunately, this data turned out to be too sparse for our tuning curve inference to work on it. This was disappointing for sure, but also gave me a lot of valuable experience working with experimental data. For the second project, which we are still working on, we are applying the same method to electrophysiological data recorded from the medial entorhinal cortex of rodents performing a different spatial navigation task. Luckily, we've been able to reuse a lot of the data analysis tools we developed for the first project on this one.

One of the main reasons that I am applying to MIT is that there are a number of investigators there whose interests I think align very well with mine. I like Prof. Ila Fiete's theoretical work studying how neural codes and memory networks are robust to noise, but I'm also interested in Prof. Mehrdad Jazayeri's more experimentally-oriented work investigating neural coding and dynamics. Prof. Joshua Tenebaum's work modeling learning and induction using the framework of statistical inference also greatly interests me. Because of the opportunity to work with researchers like these, as well as the opportunity to explore the insights other related fields have to offer while still building a solid base of knowledge in my own discipline, I believe MIT would be an exceptional place to pursue my PhD in neuroscience.
